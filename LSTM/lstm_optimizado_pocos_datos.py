# lstm_optimizado_pocos_datos.py
# LSTM espec√≠ficamente dise√±ado para datasets muy peque√±os
# Soluciona problemas identificados en resultados previos

import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import datetime
import json
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import StratifiedKFold, LeaveOneOut
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report

try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import Dense, Dropout, LSTM, Conv1D, GlobalMaxPooling1D
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.callbacks import EarlyStopping
    from tensorflow.keras.regularizers import l1_l2
    
    tf.get_logger().setLevel('ERROR')
    os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
    KERAS_AVAILABLE = True
    
except ImportError:
    KERAS_AVAILABLE = False
    print("‚ùå TensorFlow no disponible")

class LSTMPocosData:
    """
    LSTM optimizado espec√≠ficamente para datasets muy peque√±os
    Soluciona problemas de overfitting y pocos datos
    """
    
    def __init__(self, directorio_base="todo/firmas_espectrales_csv"):
        self.directorio_base = directorio_base
        
        # Configuraci√≥n para pocos datos
        self.min_samples_warning = 20  # Advertir si hay menos de 20 muestras
        self.max_epochs_small_data = 50  # M√°ximo epochs para datos peque√±os
        
        # Configuraci√≥n para reproducibilidad
        np.random.seed(42)
        if KERAS_AVAILABLE:
            tf.random.set_seed(42)
    
    def cargar_firma_espectral_optimizada(self, contaminante):
        """
        Carga y optimiza firma espectral para pocos datos
        """
        print(f"\nüìä Cargando firma espectral: {contaminante}")
        
        # Construir ruta
        ruta_carpeta = os.path.join(self.directorio_base, contaminante)
        archivos_espectrales = [f for f in os.listdir(ruta_carpeta) if f.endswith('_datos_espectrales.csv')]
        
        if not archivos_espectrales:
            raise FileNotFoundError(f"No se encontr√≥ archivo espectral en {ruta_carpeta}")
        
        # Cargar datos
        ruta_archivo = os.path.join(ruta_carpeta, archivos_espectrales[0])
        datos = pd.read_csv(ruta_archivo)
        
        # Asegurar columnas correctas
        if 'wavelength' not in datos.columns:
            # Intentar mapeo autom√°tico
            posibles_wave = [col for col in datos.columns if 'wave' in col.lower() or 'nm' in col.lower()]
            if posibles_wave:
                datos = datos.rename(columns={posibles_wave[0]: 'wavelength'})
        
        if 'high_mean' not in datos.columns:
            posibles_high = [col for col in datos.columns if 'high' in col.lower()]
            if posibles_high:
                datos = datos.rename(columns={posibles_high[0]: 'high_mean'})
        
        if 'low_mean' not in datos.columns:
            posibles_low = [col for col in datos.columns if 'low' in col.lower()]
            if posibles_low:
                datos = datos.rename(columns={posibles_low[0]: 'low_mean'})
        
        # Limpiar y ordenar
        datos = datos.dropna()
        datos = datos.sort_values('wavelength').reset_index(drop=True)
        
        print(f"   ‚úÖ Datos cargados: {datos.shape}")
        print(f"   üåà Rango: {datos['wavelength'].min():.1f}-{datos['wavelength'].max():.1f} nm")
        
        return datos
    
    def data_augmentation_agresivo(self, datos, factor_aumento=20):
        """
        Data augmentation AGRESIVO espec√≠fico para firmas espectrales
        Genera muchas m√°s muestras para combatir overfitting
        """
        print(f"üîß Data augmentation agresivo (factor: {factor_aumento})...")
        
        wavelengths = datos['wavelength'].values
        high_response = datos['high_mean'].values
        low_response = datos['low_mean'].values
        
        # Normalizar wavelengths
        wavelength_norm = (wavelengths - wavelengths.min()) / (wavelengths.max() - wavelengths.min())
        
        # Crear muestras originales
        X_original = []
        y_original = []
        
        # Alta concentraci√≥n
        for i in range(len(wavelengths)):
            X_original.append([wavelength_norm[i], high_response[i]])
            y_original.append(1)
        
        # Baja concentraci√≥n  
        for i in range(len(wavelengths)):
            X_original.append([wavelength_norm[i], low_response[i]])
            y_original.append(0)
        
        X_original = np.array(X_original)
        y_original = np.array(y_original)
        
        # Data augmentation con m√∫ltiples t√©cnicas
        X_augmented = [X_original]
        y_augmented = [y_original]
        
        for aug_iter in range(factor_aumento):
            X_aug = X_original.copy()
            
            # T√©cnica 1: Ruido gaussiano adaptativo
            noise_level = np.random.uniform(0.01, 0.05)
            noise = np.random.normal(0, noise_level, X_aug.shape)
            X_aug += noise
            
            # T√©cnica 2: Escalado de respuesta espectral
            scale_factor = np.random.uniform(0.9, 1.1)
            X_aug[:, 1] *= scale_factor
            
            # T√©cnica 3: Desplazamiento espectral
            shift = np.random.uniform(-0.02, 0.02)
            X_aug[:, 0] += shift
            X_aug[:, 0] = np.clip(X_aug[:, 0], 0, 1)
            
            # T√©cnica 4: Interpolaci√≥n entre muestras
            if aug_iter % 5 == 0:  # Cada 5 iteraciones
                alpha = np.random.uniform(0.2, 0.8)
                indices = np.random.choice(len(X_original), len(X_original))
                X_aug = alpha * X_aug + (1 - alpha) * X_original[indices]
            
            # T√©cnica 5: Perturbaci√≥n de baseline
            if aug_iter % 3 == 0:  # Cada 3 iteraciones
                baseline_shift = np.random.uniform(-0.01, 0.01)
                X_aug[:, 1] += baseline_shift
            
            X_augmented.append(X_aug)
            y_augmented.append(y_original)
        
        # Combinar todo
        X_final = np.vstack(X_augmented)
        y_final = np.hstack(y_augmented)
        
        # Crear secuencias para LSTM
        n_samples_por_clase = len(X_final) // (2 * len(wavelengths))
        n_timesteps = len(wavelengths)
        
        X_sequences = X_final.reshape(n_samples_por_clase * 2, n_timesteps, 2)
        y_sequences = y_final[::n_timesteps]
        
        print(f"   üìà Muestras originales: 2")
        print(f"   üìà Muestras aumentadas: {len(X_sequences)}")
        print(f"   üìä Secuencias LSTM: {X_sequences.shape}")
        print(f"   üéØ Distribuci√≥n: {np.bincount(y_sequences.astype(int))}")
        
        return X_sequences, y_sequences, wavelengths
    
    def crear_modelo_minimalista(self, input_shape, regularization_strength=0.01):
        """
        Crea modelo LSTM MINIMALISTA para pocos datos
        M√°xima regularizaci√≥n, m√≠nima complejidad
        """
        print(f"üß† Creando modelo minimalista para pocos datos...")
        print(f"   üìê Input shape: {input_shape}")
        
        model = Sequential()
        
        # OPCI√ìN 1: LSTM Ultra-simple
        model.add(LSTM(
            16,  # Solo 16 unidades 
            return_sequences=False,
            input_shape=input_shape,
            dropout=0.5,  # Dropout alto
            recurrent_dropout=0.5,
            kernel_regularizer=l1_l2(l1=regularization_strength, l2=regularization_strength),
            recurrent_regularizer=l1_l2(l1=regularization_strength, l2=regularization_strength)
        ))
        
        # Dropout agresivo
        model.add(Dropout(0.6))
        
        # Capa densa m√≠nima
        model.add(Dense(
            8, 
            activation='relu',
            kernel_regularizer=l1_l2(l1=regularization_strength, l2=regularization_strength)
        ))
        model.add(Dropout(0.5))
        
        # Salida
        model.add(Dense(1, activation='sigmoid'))
        
        # Compilar con learning rate bajo
        model.compile(
            optimizer=Adam(learning_rate=0.0001),  # LR muy bajo
            loss='binary_crossentropy',
            metrics=['accuracy']
        )
        
        print(f"   ‚úÖ Par√°metros totales: {model.count_params():,}")
        
        return model
    
    def validacion_cruzada_robusta(self, X, y, n_splits=5):
        """
        Validaci√≥n cruzada robusta para datasets peque√±os
        """
        print(f"üîÑ Validaci√≥n cruzada robusta ({n_splits} folds)...")
        
        # Si hay muy pocos datos, usar Leave-One-Out
        if len(X) < 10:
            print("   ‚ö†Ô∏è Datos insuficientes, usando Leave-One-Out")
            cv = LeaveOneOut()
            n_splits = len(X)
        else:
            cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)
        
        scores = []
        
        for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):
            print(f"   üìÇ Fold {fold + 1}/{min(n_splits, len(X))}")
            
            X_train_fold, X_val_fold = X[train_idx], X[val_idx]
            y_train_fold, y_val_fold = y[train_idx], y[val_idx]
            
            # Normalizar
            scaler = RobustScaler()  # M√°s robusto que StandardScaler
            X_train_scaled = X_train_fold.copy()
            X_val_scaled = X_val_fold.copy()
            
            # Escalar cada feature por separado
            for i in range(X_train_fold.shape[2]):
                X_flat_train = X_train_fold[:, :, i].reshape(-1, 1)
                X_flat_val = X_val_fold[:, :, i].reshape(-1, 1)
                
                X_scaled_train = scaler.fit_transform(X_flat_train)
                X_scaled_val = scaler.transform(X_flat_val)
                
                X_train_scaled[:, :, i] = X_scaled_train.reshape(X_train_fold.shape[0], X_train_fold.shape[1])
                X_val_scaled[:, :, i] = X_scaled_val.reshape(X_val_fold.shape[0], X_val_fold.shape[1])
            
            # Crear modelo
            modelo = self.crear_modelo_minimalista(X_train_scaled.shape[1:])
            
            # Entrenar con POCAS epochs
            history = modelo.fit(
                X_train_scaled, y_train_fold,
                validation_data=(X_val_scaled, y_val_fold),
                epochs=30,  # Pocas epochs
                batch_size=min(8, len(X_train_scaled) // 2),
                callbacks=[EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)],
                verbose=0
            )
            
            # Evaluar
            y_pred_proba = modelo.predict(X_val_scaled, verbose=0)
            y_pred = (y_pred_proba > 0.5).astype(int).flatten()
            
            # M√©tricas
            acc = accuracy_score(y_val_fold, y_pred)
            f1 = f1_score(y_val_fold, y_pred, average='binary', zero_division=0)
            
            try:
                auc = roc_auc_score(y_val_fold, y_pred_proba)
            except:
                auc = 0.5
            
            scores.append({
                'accuracy': acc,
                'f1_score': f1,
                'auc': auc,
                'epochs_trained': len(history.history['loss'])
            })
            
            print(f"      üìä Acc: {acc:.3f}, F1: {f1:.3f}, AUC: {auc:.3f}")
        
        # Promediar resultados
        avg_scores = {
            'accuracy': np.mean([s['accuracy'] for s in scores]),
            'f1_score': np.mean([s['f1_score'] for s in scores]),
            'auc': np.mean([s['auc'] for s in scores]),
            'accuracy_std': np.std([s['accuracy'] for s in scores]),
            'f1_score_std': np.std([s['f1_score'] for s in scores]),
            'auc_std': np.std([s['auc'] for s in scores]),
            'fold_scores': scores
        }
        
        print(f"\nüìà RESULTADOS PROMEDIO:")
        print(f"   üéØ Accuracy: {avg_scores['accuracy']:.4f} ¬± {avg_scores['accuracy_std']:.4f}")
        print(f"   üéØ F1-score: {avg_scores['f1_score']:.4f} ¬± {avg_scores['f1_score_std']:.4f}")
        print(f"   üéØ AUC: {avg_scores['auc']:.4f} ¬± {avg_scores['auc_std']:.4f}")
        
        return avg_scores
    
    def entrenar_contaminante_optimizado(self, contaminante):
        """
        Entrena modelo optimizado para un contaminante con pocos datos
        """
        print(f"\n{'='*70}")
        print(f"üß¨ ENTRENANDO LSTM OPTIMIZADO: {contaminante}")
        print(f"{'='*70}")
        
        if not KERAS_AVAILABLE:
            print("‚ùå Keras no disponible")
            return None
        
        try:
            inicio_tiempo = datetime.datetime.now()
            
            # 1. Cargar datos
            datos_espectrales = self.cargar_firma_espectral_optimizada(contaminante)
            
            # 2. Data augmentation agresivo
            X_sequences, y_sequences, wavelengths = self.data_augmentation_agresivo(
                datos_espectrales, factor_aumento=50  # MUY agresivo
            )
            
            # 3. Advertir si hay pocos datos
            if len(X_sequences) < self.min_samples_warning:
                print(f"‚ö†Ô∏è ADVERTENCIA: Solo {len(X_sequences)} muestras despu√©s de augmentation")
                print("   Considera usar modelos m√°s simples (SVM, Random Forest)")
            
            # 4. Validaci√≥n cruzada robusta
            resultados_cv = self.validacion_cruzada_robusta(X_sequences, y_sequences)
            
            fin_tiempo = datetime.datetime.now()
            tiempo_entrenamiento = (fin_tiempo - inicio_tiempo).total_seconds()
            
            # 5. Preparar resultados finales
            resultados = {
                'contaminante': contaminante,
                'metodo': 'lstm_optimizado_pocos_datos',
                'accuracy': resultados_cv['accuracy'],
                'f1_score': resultados_cv['f1_score'],
                'auc': resultados_cv['auc'],
                'accuracy_std': resultados_cv['accuracy_std'],
                'f1_score_std': resultados_cv['f1_score_std'],
                'auc_std': resultados_cv['auc_std'],
                'tiempo_entrenamiento': tiempo_entrenamiento,
                'n_muestras_aumentadas': len(X_sequences),
                'n_wavelengths': len(wavelengths),
                'metodo_validacion': 'validacion_cruzada_robusta',
                'fold_scores': resultados_cv['fold_scores']
            }
            
            # 6. Clasificar rendimiento
            f1_score = resultados['f1_score']
            if f1_score >= 0.80:
                clasificacion = "ü•á EXCELENTE"
            elif f1_score >= 0.70:
                clasificacion = "ü•à BUENO"
            elif f1_score >= 0.60:
                clasificacion = "ü•â ACEPTABLE"
            else:
                clasificacion = "‚ùå PROBLEM√ÅTICO"
            
            print(f"\n{'='*50}")
            print(f"üìä RESULTADO FINAL - {contaminante}")
            print(f"{'='*50}")
            print(f"üéØ F1-score: {f1_score:.4f} ¬± {resultados['f1_score_std']:.4f}")
            print(f"üéØ Accuracy: {resultados['accuracy']:.4f} ¬± {resultados['accuracy_std']:.4f}")
            print(f"üéØ AUC: {resultados['auc']:.4f} ¬± {resultados['auc_std']:.4f}")
            print(f"‚è±Ô∏è Tiempo: {tiempo_entrenamiento:.1f}s")
            print(f"üìà Clasificaci√≥n: {clasificacion}")
            
            return resultados
            
        except Exception as e:
            print(f"‚ùå Error: {str(e)}")
            import traceback
            traceback.print_exc()
            return None
    
    def entrenar_todos_optimizado(self, contaminantes_test=None):
        """
        Entrena modelos optimizados para varios contaminantes
        """
        if contaminantes_test is None:
            # Detectar contaminantes autom√°ticamente
            contaminantes_test = []
            for carpeta in os.listdir(self.directorio_base):
                ruta_carpeta = os.path.join(self.directorio_base, carpeta)
                if os.path.isdir(ruta_carpeta):
                    archivos = [f for f in os.listdir(ruta_carpeta) if f.endswith('_datos_espectrales.csv')]
                    if archivos:
                        contaminantes_test.append(carpeta)
        
        print(f"\n{'='*80}")
        print(f"üî¨ ENTRENAMIENTO LSTM OPTIMIZADO - DATASET PEQUE√ëO")
        print(f"{'='*80}")
        print(f"üìã Contaminantes: {len(contaminantes_test)}")
        print(f"üõ†Ô∏è Optimizaciones: Data augmentation agresivo, modelo minimalista, validaci√≥n robusta")
        
        resultados_todos = {}
        inicio_total = datetime.datetime.now()
        
        for i, contaminante in enumerate(contaminantes_test):
            print(f"\n[{i+1}/{len(contaminantes_test)}] üîÑ Procesando: {contaminante}")
            
            resultado = self.entrenar_contaminante_optimizado(contaminante)
            resultados_todos[contaminante] = resultado
        
        fin_total = datetime.datetime.now()
        tiempo_total = (fin_total - inicio_total).total_seconds()
        
        # Generar reporte comparativo
        self._generar_reporte_optimizado(resultados_todos, tiempo_total)
        
        return resultados_todos
    
    def _generar_reporte_optimizado(self, resultados, tiempo_total):
        """Genera reporte optimizado comparando con resultados previos"""
        
        print(f"\n{'='*80}")
        print(f"üìä REPORTE OPTIMIZADO - COMPARACI√ìN CON RESULTADOS PREVIOS")
        print(f"{'='*80}")
        
        # Filtrar resultados exitosos
        exitosos = {k: v for k, v in resultados.items() if v is not None}
        
        if not exitosos:
            print("‚ùå No hay resultados exitosos")
            return
        
        # Estad√≠sticas del modelo optimizado
        f1_scores = [r['f1_score'] for r in exitosos.values()]
        accuracies = [r['accuracy'] for r in exitosos.values()]
        aucs = [r['auc'] for r in exitosos.values()]
        
        # Estad√≠sticas previas (del contexto anterior)
        f1_promedio_anterior = 0.348  # Del resultado anterior
        accuracy_promedio_anterior = 0.523
        auc_promedio_anterior = 0.591
        
        # Mejoras
        mejora_f1 = np.mean(f1_scores) - f1_promedio_anterior
        mejora_accuracy = np.mean(accuracies) - accuracy_promedio_anterior
        mejora_auc = np.mean(aucs) - auc_promedio_anterior
        
        print(f"‚úÖ Modelos exitosos: {len(exitosos)}")
        print(f"‚è±Ô∏è Tiempo total: {tiempo_total:.1f}s")
        print(f"\nüìà COMPARACI√ìN DE RENDIMIENTO:")
        print(f"   F1-score:")
        print(f"     ‚Ä¢ Anterior: {f1_promedio_anterior:.4f}")
        print(f"     ‚Ä¢ Optimizado: {np.mean(f1_scores):.4f} ¬± {np.std(f1_scores):.4f}")
        print(f"     ‚Ä¢ Mejora: {mejora_f1:+.4f} ({'‚úÖ' if mejora_f1 > 0 else '‚ùå'})")
        
        print(f"   Accuracy:")
        print(f"     ‚Ä¢ Anterior: {accuracy_promedio_anterior:.4f}")
        print(f"     ‚Ä¢ Optimizado: {np.mean(accuracies):.4f} ¬± {np.std(accuracies):.4f}")
        print(f"     ‚Ä¢ Mejora: {mejora_accuracy:+.4f} ({'‚úÖ' if mejora_accuracy > 0 else '‚ùå'})")
        
        print(f"   AUC:")
        print(f"     ‚Ä¢ Anterior: {auc_promedio_anterior:.4f}")
        print(f"     ‚Ä¢ Optimizado: {np.mean(aucs):.4f} ¬± {np.std(aucs):.4f}")
        print(f"     ‚Ä¢ Mejora: {mejora_auc:+.4f} ({'‚úÖ' if mejora_auc > 0 else '‚ùå'})")
        
        # Top 5 mejores
        print(f"\nüèÜ TOP 5 CONTAMINANTES:")
        ranking = sorted(exitosos.items(), key=lambda x: x[1]['f1_score'], reverse=True)
        
        for i, (contaminante, resultado) in enumerate(ranking[:5], 1):
            f1 = resultado['f1_score']
            f1_std = resultado['f1_score_std']
            
            if f1 >= 0.8:
                emoji = "ü•á"
            elif f1 >= 0.7:
                emoji = "ü•à"
            elif f1 >= 0.6:
                emoji = "ü•â"
            else:
                emoji = "üìä"
            
            print(f"   {i}. {contaminante:15} | F1: {f1:.4f}¬±{f1_std:.3f} {emoji}")
        
        # Guardar resultados
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        nombre_archivo = f"lstm_optimizado_{timestamp}.json"
        
        resumen = {
            'timestamp': datetime.datetime.now().isoformat(),
            'metodo': 'lstm_optimizado_pocos_datos',
            'mejoras_implementadas': [
                'Data augmentation agresivo (factor 50x)',
                'Modelo minimalista (16 LSTM units)',
                'Regularizaci√≥n fuerte (L1+L2 + Dropout 0.5-0.6)',
                'Validaci√≥n cruzada robusta',
                'Learning rate bajo (0.0001)'
            ],
            'tiempo_total_segundos': tiempo_total,
            'comparacion_con_anterior': {
                'f1_score_anterior': f1_promedio_anterior,
                'f1_score_optimizado': float(np.mean(f1_scores)),
                'mejora_f1': float(mejora_f1),
                'accuracy_anterior': accuracy_promedio_anterior,
                'accuracy_optimizado': float(np.mean(accuracies)),
                'mejora_accuracy': float(mejora_accuracy)
            },
            'estadisticas': {
                'exitosos': len(exitosos),
                'f1_score_promedio': float(np.mean(f1_scores)),
                'f1_score_std': float(np.std(f1_scores)),
                'accuracy_promedio': float(np.mean(accuracies)),
                'auc_promedio': float(np.mean(aucs))
            },
            'resultados_detallados': exitosos
        }
        
        try:
            with open(nombre_archivo, 'w', encoding='utf-8') as f:
                json.dump(resumen, f, indent=2, ensure_ascii=False)
            print(f"\nüíæ Resultados guardados: {nombre_archivo}")
        except Exception as e:
            print(f"‚ö†Ô∏è Error guardando: {e}")
        
        return resumen

def main_optimizado():
    """Funci√≥n principal del LSTM optimizado"""
    
    if not KERAS_AVAILABLE:
        print("‚ùå TensorFlow no disponible")
        return
    
    print("üîß LSTM OPTIMIZADO PARA DATASETS PEQUE√ëOS")
    print("="*60)
    print("üéØ Soluciona problemas identificados:")
    print("   ‚Ä¢ Overfitting severo")
    print("   ‚Ä¢ Dataset extremadamente peque√±o") 
    print("   ‚Ä¢ Arquitectura demasiado compleja")
    print("   ‚Ä¢ Validaci√≥n inadecuada")
    
    # Crear entrenador optimizado
    entrenador = LSTMPocosData("todo/firmas_espectrales_csv")
    
    # Probar con algunos contaminantes primero
    contaminantes_prueba = ["Caffeine", "Acesulfame", "Doc", "Nh4", "Turbidity"]
    
    print(f"\nüß™ Probando con {len(contaminantes_prueba)} contaminantes...")
    
    resultados = entrenador.entrenar_todos_optimizado(contaminantes_prueba)
    
    return resultados

if __name__ == "__main__":
    resultados = main_optimizado()